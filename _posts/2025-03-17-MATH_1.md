---
permalink: /DLML/
title: "Log-likelihood"
layout: single
date: 2025-03-17
categories: [blog]
tags: [Blog]
sidebar_main: true
toc: true
toc_sticky: true
toc_label: 목차
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# Log Probability

일반적인 확률의 범위는 $$ [0, 1] $$이며, 이를 로그 변환하면 $$(- \infty, 0]$$ 범위로 바뀐다.  
로그 확률은 다음과 같은 장점을 가진다.

- **Stability (안정성)**
  - 확률을 곱하는 연산을 반복하면 값이 매우 작아져서 수치적으로 불안정해질 수 있다.
  - 한정된 정밀도(finite precision approximation) 환경에서는 이로 인해 연산 오류가 발생할 수 있다.
  
- **Simplification (단순화)**
  - 로그 변환을 통해 확률 밀도 함수의 연산을 단순화할 수 있다.
  - 예를 들어, 확률 밀도 함수가 지수 분포군(exponential family)에 속해 있을 때, 로그를 취하면 gradient 계산이 더 쉬워진다.

---

# Negative Log Likelihood (NLL)

주어진 입력 $$x$$를 기반으로 모델 $$f_\theta$$가 실제값 $$y$$와 가장 일치하는 예측값 $$\hat{y}$$를 생성하도록 학습하는 것이 목표이다.  

- $$\sigma$$는 비선형 활성화 함수이며, 일반적으로 sigmoid를 사용하여 값을 $$[0,1]$$ 범위로 매핑한다.
- 예측값과 실제값이 일치하도록 likelihood를 최대화해야 한다.

### 모델의 예측값:
$$
\hat{y_{\theta ,i}}=\sigma (f_\theta(x_i))
$$

### Likelihood 함수:
$$
\mathbb{P}(D\mid \theta )=\prod_{i=1}^{n} \hat{y}_{\theta ,i}^{y_i} (1-\hat{y}_{\theta ,i})^{1-{y_i}}
$$

## Maximum Likelihood Estimation (MLE)

위 식에서 $$\prod$$는 확률을 곱하는 연산을 나타낸다.  
Binary classification(sigmoid activation)에서 likelihood의 동작 방식은 다음과 같다.

- **$$( y_i = 0 )$$일 때**  
  - 첫 번째 항 $$\hat{y}_{\theta ,i}^{y_i}$$에서 $$( y_i = 0 )$$이므로 값이 1이 되고,  
  - 두 번째 항 $$(1-\hat{y}_{\theta ,i})^{1-{y_i}}$$만 남는다.
  - 즉, 모델이 $$( y_i = 0 )$$을 정확하게 예측할수록 likelihood가 1에 가까워진다.

- **$$( y_i = 1 )$$일 때**  
  - 첫 번째 항 $$\hat{y}_{\theta ,i}$$만 남고,  
  - 두 번째 항은 1이 된다.
  - 즉, 모델이 $$( y_i = 1 )$$을 정확하게 예측할수록 likelihood가 1에 가까워진다.

## Log-Likelihood

Likelihood 함수에 로그를 취하면, 곱셈이 덧셈으로 변하여 연산이 용이해진다.

$$
\log \mathbb{P}(D\mid \theta )=\sum_{i=1}^{n} ( y_i \log \hat{y}_{\theta ,i} + (1-y_i) \log(1-\hat{y}_{\theta ,i}) )
$$

## Minimize Negative Log-Likelihood (NLL)

최대 가능도(Log-Likelihood)의 값을 크게 만드는 것이 목적이지만,  
Gradient Descent를 적용하려면 손실(loss)을 최소화하는 방향으로 학습을 진행해야 한다.  
따라서 Log-Likelihood의 부호를 반대로 적용한 Negative Log-Likelihood(NLL) Loss를 사용한다.

$$
l(\theta )=-\sum_{i=1}^{n} ( y_i \log \hat{y}_{\theta ,i} + (1-y_i) \log(1-\hat{y}_{\theta ,i}) )
$$

---

# Cross-Entropy Loss

$$
H(p,q) = -\sum_{x\in \chi }^{} p(x) \log q(x)
$$

Cross-Entropy Loss는 모델의 예측값을 Softmax를 통해 확률로 변환한 후,  
이 확률과 실제 정답 간의 거리를 측정하여 모델이 학습할 수 있도록 한다.  
엔트로피(Entropy)는 불확실성을 측정하는 개념이며, 값이 클수록 확률 분포의 불확실성이 크다는 것을 의미한다.

## Categorical Cross-Entropy vs. Sparse Categorical Cross-Entropy

- **Categorical Cross-Entropy**  
  - 실제 정답(label)이 **one-hot encoding** 형태로 표현된다.
  
- **Sparse Categorical Cross-Entropy**  
  - 실제 정답(label)이 **정수(integer label)** 로 표현된다.

두 손실 함수 모두 multi-class classification에 적용 가능하며, 데이터 형식에 따라 선택하면 된다.

---

## Cross-Entropy vs. Negative Log-Likelihood

- **정의적 차이**  
  - 수학적으로는 NLL과 CE는 동일하다.
  - 그러나 PyTorch에서는 다르게 구현되어 있다.

- **PyTorch 구현 차이**  
  - `CrossEntropyLoss`는 **Softmax → log 변환**을 자동으로 수행한다.
  - `NLLLoss`는 log probability를 입력으로 받으므로, Softmax 변환이 포함되지 않는다.
  - 즉, `CrossEntropyLoss`는 raw logits을 입력받고, `NLLLoss`는 log probability를 입력받는다.

---

# Reference

> 1. https://velog.io/@mmodestaa/Log-Probability-Negative-Log-Likelihood-and-Cross-Entropy-%EC%84%A4%EB%AA%85 
> 2. https://supermemi.tistory.com/entry/Loss-Cross-Entropy-Negative-Log-Likelihood-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC-Pytorch-Code
