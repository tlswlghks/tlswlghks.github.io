---
permalink: /MU_2/
title: "Machine Unlearning of Pre-trained Large Language Models (2024)"
layout: single
date: 2025-03-17
categories: [blog]
tags: [Blog]
sidebar_main: true
toc: true
toc_sticky: true
toc_label: ëª©ì°¨
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Abstract
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¼ê³± ê°€ì§€ ë‹¤ì–‘í•œ unlearning ë°©ë²•ì„ ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , pre-trained LLMì˜ machine unlearningì„ ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•œë‹¤.

ë˜í•œ, ë¶„í¬ ë‚´ ë°ì´í„°ì—ì„œ gradient ascentì™€ gradient descentë¥¼ í†µí•©í•˜ë©´ hyperparameterì˜ ê²¬ê³ ì„±ì´ í–¥ìƒë¨ì„ ë³´ì¸ë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íš¨ìœ¨ì ì¸ hyperparameter íŠœë‹ì„ ìœ„í•œ ìì„¸í•œ ê°€ì´ë“œë¥¼ ì œê³µí•˜ì—¬, pre-trained LLMì˜ machine unlearning ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ í†µì°°ë ¥ì„ ì œì‹œí•˜ê³  ì±…ì„ê° ìˆëŠ” AI ê°œë°œì˜ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•œë‹¤.

---

## Introduction
LLMì€ ë°©ëŒ€í•œ ë°ì´í„° í’€ì„ í™œìš©í•˜ì§€ë§Œ, ì´ ë°ì´í„°ê°€ ë¯¼ê°í•˜ê±°ë‚˜(sensitive), ì‚¬ì ì´ê±°ë‚˜(private), ì €ì‘ê¶Œì´ ìˆëŠ”(copyrighted) ê²½ìš° ìœ¤ë¦¬ì  ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, **The New York Times**ëŠ” OpenAIê°€ ChatGPTë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë° ìì‚¬ì˜ ìˆ˜ë°±ë§Œ ê°œ ê¸°ì‚¬ë¥¼ ë¬´ë‹¨ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤ë©° ì†Œì†¡ì„ ì œê¸°í–ˆë‹¤. ì´ëŠ” LLM ê°œë°œì—ì„œ ì €ì‘ê¶Œ ì¹¨í•´ ë¬¸ì œë¥¼ ë¶€ê°ì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ì‚¬ë¡€ë‹¤.

ì´ëŸ¬í•œ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ì•ˆìœ¼ë¡œ **machine unlearning**ì´ ë“±ì¥í–ˆë‹¤. machine unlearningì€ ëª¨ë¸ì´ íŠ¹ì • ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì€ ê²ƒì²˜ëŸ¼ ë™ì‘í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, LLMì˜ ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì—ì„œ ë¹„ë¡¯ëœ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ LLMì´ ë³€í™”í•˜ëŠ” ë²•ì  ë° ìœ¤ë¦¬ì  ê¸°ì¤€ì„ ì¤€ìˆ˜í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ê·¸ëŸ¬ë‚˜ í˜„ì¬ machine unlearning ì—°êµ¬ëŠ” ì£¼ë¡œ fine-tuned ëª¨ë¸ì— ì§‘ì¤‘ë˜ì–´ ìˆë‹¤. ë³¸ ë…¼ë¬¸ì€ **pre-trained LLMì˜ unlearning**ì— ì´ˆì ì„ ë§ì¶”ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ë„ì „ ê³¼ì œë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤.

1. ê¸°ì¡´ì˜ unlearning ë°©ë²•ì„ pre-trained LLMì— ì ìš©í•´ì•¼ í•  í•„ìš”ì„±
2. LLM ê°œë°œì— ì‚¬ìš©ëœ pre-trained ë°ì´í„°ê°€ ê³µê°œì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•ŠëŠ” ë¬¸ì œ
3. pre-trained LLMì„ ë‹¤ì‹œ í•™ìŠµ(retrain)í•˜ëŠ” ë° ë“œëŠ” ë§‰ëŒ€í•œ ë¹„ìš©

ë³¸ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬(contributions)ì™€ ë°œê²¬ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
- LLMì„ ìœ„í•œ í†µí•© unlearning í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì„±í•˜ê³ , 7ê°€ì§€ unlearning ë°©ë²•ë¡ ì„ ë„ì¶œí•˜ì—¬ LLMì— ì ìš©
- Pre-trained LLMì„ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë¹„í˜„ì‹¤ì ì´ë¯€ë¡œ, ì´ë¥¼ ìš°íšŒí•  ìˆ˜ ìˆëŠ” approximate retraining baselineì„ ì œì•ˆí•˜ê³  ì‹¤í—˜ì„ í†µí•´ ê²€ì¦
- Gradient ascentì™€ gradient descentë¥¼ ê²°í•©í•˜ì—¬ hyperparameterì˜ ê²¬ê³ ì„±ì„ ë†’ì„
- íš¨ìœ¨ì ì¸ hyperparameter íŠœë‹ì„ ìœ„í•œ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•˜ì—¬ LLMì˜ unlearningì„ ë”ìš± ì‹¤í˜„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

---

## Problem Formulation
í›ˆë ¨ ë°ì´í„°ì…‹ì„ $$D=\{x_i\}_{i=1}^{N}$$ ë¼ê³  ì •ì˜í•˜ë©°, ì—¬ê¸°ì„œ $$x_{i}$$ëŠ” $$t_i$$ ê°œì˜ í† í° $$w_{1}^{i}, w_{2}^{i}, ..., w_{t_i}^{i}$$ë¡œ êµ¬ì„±ëœ ì‹œí€€ìŠ¤ì´ë‹¤.

Generative LLM $$M$$ì€ ì¼ë°˜ì ìœ¼ë¡œ **next-token prediction**ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë˜ë©°, ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í† í°ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ëª¨ë¸ë§í•œë‹¤.

ë˜í•œ, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ $$A$$ë¼ê³  ì •ì˜í•˜ê³ , ëª¨ë¸ì„ $$M \leftarrow A(D)$$ë¡œ í‘œí˜„í•œë‹¤. ì´ë•Œ í•™ìŠµ ëª©ì ì€ **negative log-likelihood**ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

$$ L(P_M;D)=-\sum_{x_i\in D} \sum_{t=1}^{t_i} \log P_M(w_{t+1}^i | w_1^i, ..., w_t^i) $$

unlearningì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” íŠ¹ì • ë°ì´í„°ë¥¼ **forget set** $$u \subset D$$ ë¼ê³  ì •ì˜í•˜ê³ , ì´ë¥¼ ì œê±°í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ $$M'$$ì´ë¼ê³  í•œë‹¤. ì¦‰, **unlearned model** $$M'$$ì€ unlearning ì•Œê³ ë¦¬ì¦˜ $$\hat{A}$$ë¥¼ í†µí•´ ìƒì„±ëœë‹¤.

ë¹„ê³µì‹ì ìœ¼ë¡œ unlearningì´ ì„±ê³µí•˜ë ¤ë©´, $$M'$$ì˜ ì¶œë ¥ ë¶„í¬ê°€ forget setì´ ì—†ëŠ” ëª¨ë¸ $$M^*$$ê³¼ ìœ ì‚¬í•´ì•¼ í•œë‹¤. ì´ë¥¼ **exact unlearning**ì´ë¼ê³  í•œë‹¤.

$$ \mathbb{E}P_{M^*} \approx \mathbb{E}P_{M'} \quad \text{where} \quad M^* \leftarrow A(D\setminus u) $$

ì¦‰, forget set $$u$$ê°€ ì œê±°ëœ ë°ì´í„°ì…‹ $$D\setminus u$$ë¡œ í•™ìŠµí•œ ëª¨ë¸ $$M^*$$ê³¼, unlearning ì•Œê³ ë¦¬ì¦˜ì´ ì ìš©ëœ ëª¨ë¸ $$M'$$ì˜ ì„±ëŠ¥ì´ ìœ ì‚¬í•´ì•¼ í•œë‹¤. ë˜í•œ, forget set $$u$$ì™€ unseen datasetì—ì„œ $$M'$$ì˜ ì„±ëŠ¥ì€ vanilla modelë³´ë‹¤ ë‚®ì•„ì•¼ í•œë‹¤.

ì´ê²ƒì´ **machine unlearningì˜ ê¶ê·¹ì ì¸ ëª©í‘œ**ë‹¤.

---

## Unlearning Methods

### Overview

LLMì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ í† í° ì‹œí€€ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œë„ ìœ ì§€í•´ì•¼ í•  ë°ì´í„°(retain set)ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ëŠ” ê²ƒì´ë‹¤.  
ì´ë¥¼ ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **next-token predictionì„ ì‚¬ìš©í•˜ëŠ” LLMì˜ unlearningì„ ê·¼ì‚¬í•˜ëŠ” í”„ë ˆì„ì›Œí¬**ë¥¼ ì œì•ˆí•œë‹¤.

ì•„ë˜ ìˆ˜ì‹ì€ unlearned sequences $$u$$ì—ì„œ **gradient derived**ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ëª¨ë¸ $$M$$ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤€ë‹¤.

$$
\sum_{w\in u}^{}\sum_{t=1}^{T}\mathbb{E}_{q_t \sim Q_{w_t}} \log P_M(q_t \mid  w_1, w_2, ..., w_{t-1}) + \sum_{z\in \Re }^{}\sum_{t=1}^{T}\log P_M(z_t\mid z_1, z_2, ..., z_{t-1})
$$

- $$\Re \subseteq D\setminus u$$ì´ë©°,  
- $$Q_{w_t}$$ëŠ” reference distributionìœ¼ë¡œ, $$w_t$$ì— ì˜ì¡´í•˜ëŠ” token universe $$W$$ì— ëŒ€í•œ distribution ì§‘í•©ì´ë‹¤.

> - **ì²« ë²ˆì§¸ í•­**: ìŠì–´ì•¼ í•  ë°ì´í„° $$u$$ì™€ ê´€ë ¨ë˜ë©°, $$w$$ëŠ” unlearning ëŒ€ìƒ sequenceë¥¼ ì˜ë¯¸í•œë‹¤.  
>   ì´ë¥¼ í†µí•´ **ëª¨ë¸ì´ $$u$$ì˜ ì •ë³´ë¥¼ ìŠë„ë¡ í•™ìŠµ**í•œë‹¤.  
> - **ë‘ ë²ˆì§¸ í•­**: ìœ ì§€í•´ì•¼ í•  ë°ì´í„° $$\Re $$ê³¼ ê´€ë ¨ë˜ë©°, $$z$$ëŠ” ìœ ì§€í•´ì•¼ í•˜ëŠ” sequenceë¥¼ ì˜ë¯¸í•œë‹¤.  
>   ì´ë¥¼ í†µí•´ **ëª¨ë¸ì´ $$\Re$$ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìœ ì§€**í•˜ë„ë¡ í•™ìŠµí•œë‹¤.  
> - ì´ ë‘ í•­ì„ í•©í•˜ì—¬, **ëª¨ë¸ì´ unlearningê³¼ retain set ì„±ëŠ¥ ìœ ì§€ë¼ëŠ” ë‘ ê°€ì§€ ëª©í‘œë¥¼ ë™ì‹œì— ë‹¬ì„±**í•˜ë„ë¡ í•œë‹¤.

---

### Approximate Unlearning Methods (Gradient Ascent or Negative Gradient)

ìœ„ ìˆ˜ì‹ì—ì„œ  
- **ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œ**í•˜ê³ ,  
- **$$Q_{w_t}=\delta_{w_t}$$ë¡œ ë°”ê¾¼ í›„ gradientì— -1ì„ ê³±í•˜ë©´**  

â¡ **Gradient Ascent í˜¹ì€ Negative Gradient ë°©ë²•ì´ ëœë‹¤.**

> ì—¬ê¸°ì„œ, $$\delta_{w_t}$$ëŠ” $$w_t$$ì—ì„œì˜ delta í•¨ìˆ˜ë¡œ, $$q_t \sim Q_{w_t}$$ëŠ” $$q_t=w_t$$ë¥¼ ì˜ë¯¸í•˜ë©° í™•ë¥ ì€ 1ì´ë‹¤.

**ì§ê´€ì ìœ¼ë¡œ ë³´ë©´**  
- ê¸°ì¡´ ëª¨ë¸ $$M$$ì€ $$u$$ë¡œ í•™ìŠµë˜ì—ˆì§€ë§Œ,  
- ì¬í•™ìŠµëœ ëª¨ë¸ $$M_r^u$$ëŠ” $$u$$ë¥¼ ë³´ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì— **$$M$$ì˜ $$u$$ lossëŠ” $$M_r^u$$ë³´ë‹¤ ë‚®ë‹¤.**  
- ë‹¨, $$\mid u \mid$$ê°€ ì‘ì„ ê²½ìš° loss ì°¨ì´ëŠ” í¬ì§€ ì•Šë‹¤.  

í•˜ì§€ë§Œ, **ë„ˆë¬´ ë§ì€ epoch ë™ì•ˆ gradient ascentë¥¼ ìˆ˜í–‰í•˜ë©´**  
- ëª¨ë¸ $$M$$ì´ **$$D \setminus u$$ì— ëŒ€í•œ ì •ë³´ê¹Œì§€ ìŠì–´ë²„ë ¤ utilityê°€ ì €í•˜**ëœë‹¤.  
- ë”°ë¼ì„œ, ëª‡ëª‡ ì—°êµ¬ì—ì„œëŠ” **ì¼ì • íšŸìˆ˜ë§Œ gradient ascentë¥¼ ì ìš©**í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

> - ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œí•˜ê³ , **ì²« ë²ˆì§¸ í•­ì— -1ì„ ê³±í•˜ë©´** forget setì— ëŒ€í•´ **gradient ascent íš¨ê³¼**ê°€ ì ìš©ëœë‹¤.

---

### Fine-tuning with Random Labels

ìœ„ **Overview ë°©ì •ì‹ì—ì„œ ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œ**í•˜ê³   
- ê°€ëŠ¥í•œ ëª¨ë“  token ì§‘í•© $$W$$ì— ëŒ€í•´ $$Q_{w_t}$$ë¥¼ **uniform distribution**ìœ¼ë¡œ ì„¤ì •í•˜ë©´  
â¡ **random labelì„ ì‚¬ìš©í•œ fine-tuning ë°©ì‹**ì´ ëœë‹¤.

> Random label fine-tuningì˜ í•µì‹¬ ì•„ì´ë””ì–´:  
> **$u$ë¥¼ ë³´ì§€ ì•ŠëŠ” ëª¨ë¸ì´ random guessingì„ í•´ì•¼ í•œë‹¤.**

ê·¸ëŸ¬ë‚˜, ë‹¨ìˆœí•œ uniform distribution ë°©ì‹ì€ ì ì ˆí•˜ì§€ ì•Šë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´:
- ê°™ì€ sequenceê°€ ë‘ ë²ˆ ë“±ì¥í•  ê²½ìš°, í•˜ë‚˜ëŠ” **unlearned**, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” **retained** ìƒíƒœì¼ ìˆ˜ ìˆë‹¤.
- ì´ ê²½ìš°, retrainëœ ëª¨ë¸ì´ **random guessingì„ í•˜ë©´ ì•ˆ ëœë‹¤.**

### í•´ê²°ì±…:
ì„ í–‰ ì—°êµ¬ì—ì„œëŠ” $$Q_{w_t}$$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•  ê²ƒì„ ì œì•ˆí•œë‹¤.
$$
Q_{w_t} = P_{M_{rand}}(w_t \mid w_1, ..., w_{t-1})
$$
- ì—¬ê¸°ì„œ **$$M_{rand}$$ëŠ” randomly initialized model**ì´ë‹¤.
- ì´ëŠ” **$$M_{rand}$$ê°€ $$u$$ì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ fine-tuningí•˜ëŠ” ê²ƒ**ê³¼ ìœ ì‚¬í•˜ë‹¤.

> - ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œí•˜ê³ , ì²« ë²ˆì§¸ í•­ì˜ ê°€ëŠ¥í•œ ëª¨ë“  token setì— ëŒ€í•´ **random labelì„ ì‚¬ìš©**í•¨ìœ¼ë¡œì¨ forget setì„ ì œê±°í•œë‹¤.

---

### Unlearning with Adversarial Samples

ì´ ë°©ë²•ì€ ì›ë˜ classification taskë¥¼ ìœ„í•´ ì œì•ˆë˜ì—ˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” LLMì— ë§ê²Œ ì¡°ì •í•œë‹¤.  
ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ **í•˜ë‚˜ì˜ sequence $$w_1, ..., w_T$$ë§Œ unlearned ëœë‹¤ê³  ê°€ì •**í•˜ë©´,  
ê° $$t$$ì— ëŒ€í•´ **adversarial sample** $${a_t}$$ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

$$
a_t = \arg\max_{a\neq w_t} P_M(a \mid w_1, w_2, ..., w_{t-1})
$$

ì´ë•Œ, adversarial sample $${a_t}$$ëŠ”  
- $$w_t$$ì— ê°€ê¹ì§€ë§Œ,  
- ëª¨ë¸ $$M$$ì„ ê°€ì¥ **í˜¼ë€ìŠ¤ëŸ½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í† í°**ì´ë‹¤.

ìš°ë¦¬ëŠ” forget set $$u$$ì˜ ëª¨ë“  train sequenceë¥¼ unlearní•˜ê¸° ìœ„í•´,  
- Overview ìˆ˜ì‹ì˜ **ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œ**í•˜ê³ ,  
- $$Q_{w_t} = \delta_{a_t}$$ì™€ **$$\arg\max_{a\neq w_t}P_M(a \mid w_1, ..., w_{t-1})$$ë¥¼ ì‚¬ìš©í•˜ì—¬**  
- **ëª¨ë¸ $$M$$ì„ fine-tuning**í•œë‹¤.

ì¼ë°˜ì ì¸ classification taskì—ì„œëŠ”  
- ê° train sampleì— ëŒ€í•´ **$$K-1$$ê°œì˜ adversarial sample**ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ë„ ìˆì§€ë§Œ,  
- ì´ëŠ” ë¹„í˜„ì‹¤ì ì´ë¯€ë¡œ ë‹¨ìˆœí™”í•˜ì—¬ **$$a_t=\arg\max_{a\neq w_t} P_M(a \mid w_1, ..., w_{t-1})$$**ë¡œ ì„¤ì •í•œë‹¤.

> - ë§ˆì°¬ê°€ì§€ë¡œ, **ë‘ ë²ˆì§¸ í•­ì„ ë¬´ì‹œí•˜ê³  forget setì„ adversarial sampleì„ í†µí•´ ì œê±°**í•˜ëŠ” ë°©ë²•ì´ë‹¤.  
> - **Forget setê³¼ ìœ ì‚¬í•œ adversarial sampleì„ ìƒì„±í•˜ì—¬ forget setì„ ëª¨ë¸ì—ì„œ ì‚­ì œ**í•œë‹¤.

---

### Gradient Ascent + Descent ë˜ëŠ” KL Divergence on Retained Set

Overviewì˜ ìˆ˜ì‹ì—ì„œ **ì²« ë²ˆì§¸ í•­ì„ ë¬´ì‹œ**í•˜ë©´,  
â¡ Retained setì— ëŒ€í•œ **fine-tuning ì „ëµ**ì´ ëœë‹¤.

ì´ ì „ëµì„ ì‚¬ìš©í•˜ë©´,  
- $$M$$ì„ $$D \setminus u$$ì—ì„œ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìœ¼ë©°,  
- ì´ëŠ” **ì²˜ìŒë¶€í„° ë‹¤ì‹œ í›ˆë ¨í•˜ëŠ” íš¨ê³¼**ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤.

**í•˜ì§€ë§Œ ë¬¸ì œëŠ”?**  
- Pre-trained ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì—,  
- LLMì— ì ìš©í•˜ê¸°ì—ëŠ” **ë¹„í˜„ì‹¤ì **ì´ë‹¤.

### í•´ê²°ì±…: Hybrid Approach  
ì´ ë°©ë²•ì„ **ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ ** gradient ascentì™€ í†µí•©í•˜ì—¬  
- **ë‘ í•­ì„ ìµœì í™”**í•˜ì—¬ **unlearning íš¨ê³¼ì™€ utility ê°„ ê· í˜•**ì„ ë§ì¶˜ë‹¤.

ì´ë¥¼ ìœ„í•´:
1. **Direct gradient ascent** (ì§ì ‘ ê¸°ìš¸ê¸° ìƒìŠ¹)  
2. **KL-divergence constraint methods**  

ë‘ ê°€ì§€ ë°©ë²•ì„ í™œìš©í•œë‹¤.

> - ì²« ë²ˆì§¸ í•­ì„ ë¬´ì‹œí•˜ë©´ retain datasetì— ëŒ€í•œ fine-tuning ë°©ì‹ìœ¼ë¡œ ë³€ê²½ëœë‹¤.  
> - ê·¸ëŸ¬ë‚˜, retain datasetë§Œ fine-tuningí•˜ë©´ í˜„ì‹¤ì ìœ¼ë¡œ ì ìš©ì´ ì–´ë µë‹¤.  
> - ë”°ë¼ì„œ, **ë‘ í•­ì„ í•¨ê»˜ ìµœì í™”í•˜ì—¬ unlearning íš¨ê³¼ì™€ utility ê°„ ê· í˜•ì„ ë§ì¶˜ë‹¤.**

## Experiments

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **unlearned model í‰ê°€**ì— ì´ˆì ì„ ë§ì¶˜ë‹¤.

### 1. Performance Evaluation

**ğŸ”¹ Performance on the Forget Set**
- ëª¨ë¸ì€ **Forget Set**ì— ëŒ€í•´ ì •í™•í•œ ì˜ˆì¸¡ì´ ë¶ˆê°€ëŠ¥í•´ì•¼ í•˜ë©°, ë™ì¼í•œ ìˆ˜ì¤€ì˜ **Test Set**ì— ëŒ€í•œ ì„±ëŠ¥ë„ ê°ì†Œí•´ì•¼ í•œë‹¤.  

**ğŸ”¹ Performance on the Retain Set**
- ëª¨ë¸ì˜ ì„±ëŠ¥ì€ **Retain Set**ì—ì„œ ê°ì†Œí•˜ë©´ ì•ˆ ëœë‹¤.  

**ğŸ”¹ Performance on General Downstream Tasks**
- **Downstream Task**ì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ í†µí•´ **unlearning í›„ ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ ë³€í™”**ë¥¼ ë¶„ì„í•œë‹¤.
- ëª¨ë¸ì˜ ì„±ëŠ¥ì€ **unlearning ì „ê³¼ ë¹„êµí•˜ì—¬ Downstream Taskì—ì„œ ì„±ëŠ¥ ê°ì†Œê°€ ì ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ**ëœë‹¤.

---

### 2. Approximate Retraining

- **Approximate Retraining**ì€ **Membership Inference Attack**ì—ì„œ ì˜ê°ì„ ë°›ì•„ ê°œë°œëœ **LLM í‰ê°€ ë°©ë²•**ìœ¼ë¡œ,  
  **Training Dataì™€ Unseen Data ì‚¬ì´ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í‰ê°€**í•  ìˆ˜ ìˆë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **Retrained Modelì´ ê¸°ì¡´ Training Dataë³´ë‹¤ Unseen Domain-Specific Dataì— ëŒ€í•´ ì¼ê´€ëœ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒ**ì´ë¼ê³  ê°€ì„¤ì„ ì„¸ìš´ë‹¤.
- **Pre-training Dataì™€ Unlearned Dataì˜ ë¶ˆê· í˜•**ì„ ê³ ë ¤í•  ë•Œ, **Unlearned Data ë¶„í¬ ë‚´ì—ì„œ Unseen Dataì— ëŒ€í•œ Retrained Modelì˜ ì„±ëŠ¥ì€ Vanilla Modelê³¼ ìœ ì‚¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€**ëœë‹¤.
  - ì¦‰, **Unlearned Dataì˜ ì–‘ì´ Pre-trained Dataë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì ê¸° ë•Œë¬¸ì—, Retrained Modelì˜ ì„±ëŠ¥ì´ ê¸°ì¡´ ëª¨ë¸ê³¼ í° ì°¨ì´ê°€ ì—†ì„ ê²ƒ**ì´ë¼ëŠ” ì˜ë¯¸ì´ë‹¤.

**ğŸ”¹ Approximate Set êµ¬ì„±**
- **Forget Setê³¼ ë™ì¼í•œ ë„ë©”ì¸ì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘**í•˜ì—¬ **Approximate Set**ì„ êµ¬ì¶•í•œë‹¤.
- **Retrained Modelì˜ ì„±ëŠ¥ì€ Forget Setì—ì„œ ì¸¡ì •**, **Vanilla Modelì˜ ì„±ëŠ¥ì€ Approximate Setì—ì„œ ì¸¡ì •**í•œë‹¤.
- ì´ ê³¼ì •ì€ **Learning Rate, Optimization Steps ë“±ì˜ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬ Approximate Unlearningì˜ ë²”ìœ„ë¥¼ ë” ì •êµí•˜ê²Œ ì¡°ì •í•˜ëŠ” ë° ë„ì›€**ì„ ì¤„ ìˆ˜ ìˆë‹¤.

---

### 3. Membership Inference Attack

- **LLMì˜ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ Vanilla Modelì—ì„œ íŠ¹ì • Sequenceë¥¼ ì œê±°í•˜ëŠ” ê²ƒì„ í•´ì„ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤.**
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Membership Inference Attack (MIA)**ì„ ì ìš©í•˜ì—¬, íŠ¹ì • ì‹œí€€ìŠ¤ê°€ Training Datasetì—ì„œ ì œê±°ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•œë‹¤.

**ğŸ”¹ Min-K% Prob ë°©ë²• ì ìš©**
- MIA í‰ê°€ëŠ” **Min-K% Prob ë°©ë²•**ì„ ì‚¬ìš©í•œë‹¤.
- **ë¹„íšŒì› ì˜ˆì œ(Non-member sample)**ëŠ” **íšŒì› ì˜ˆì œ(Member sample)**ì™€ ë‹¬ë¦¬ **Negative Log-likelihood ê°’ì´ ë†’ì€ ì´ìƒì¹˜ ë‹¨ì–´ë¥¼ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤ëŠ” ì „ì œ**ë¡œ ì‘ë™í•œë‹¤.
- MIAì˜ íš¨ê³¼ë¥¼ í‰ê°€í•˜ëŠ” ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” **Prediction Probabilityê°€ ìµœì†Œí™”ëœ í† í°ì˜ ë¹„ìœ¨**ì´ë‹¤.
- ë‹¤ì–‘í•œ ë¹„ìœ¨ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì—¬ **ê° ëª¨ë¸ì— ëŒ€í•´ ìµœì ì˜ Detection Performanceë¥¼ ë³´ì´ëŠ” ëª¨ë¸ì„ ì„ íƒ**í•œë‹¤.

**ğŸ”¹ AUC (Area Under Curve) Metric ì ìš©**
- MIAì˜ íš¨ê³¼ëŠ” **AUC (Area Under Curve) Metric**ì„ ì‚¬ìš©í•˜ì—¬ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•œë‹¤.
  - **AUCê°€ ë†’ì„ìˆ˜ë¡** Targeted Sequenceê°€ Training Setì—ì„œ **ì—¬ì „íˆ ì‹ë³„ ê°€ëŠ¥**í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
  - **AUCê°€ 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡** Unlearning íš¨ê³¼ê°€ **ìš°ìˆ˜**í•¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.

---

### 4. Datasets

í‰ê°€ëŠ” **ì„¸ ê°€ì§€ì˜ ë‹¤ë¥¸ ì„¸íŒ…**ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

1. **arXiv Papers**  
   - 2023ë…„ 8ì›” ì´í›„ ì¶œíŒëœ **6.1K ë…¼ë¬¸ ë°ì´í„°**  
2. **GitHub Code Repositories**  
   - **2023ë…„ 11ì›” ì—…ë¡œë“œëœ 15.8K ì½”ë“œ íŒŒì¼** (permissive license í¬í•¨)  
3. **Books**  
   - **2023ë…„ ì´í›„ ì¶œíŒëœ 50ê¶Œì˜ ì±…** (BookMIAì˜ Unseen Data ê¸°ë°˜)

- ìœ„ ë°ì´í„°ì…‹ë“¤ì€ **ë¬´ë£Œë¡œ ì´ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ì „íˆ Copyright ìš°ë ¤ê°€ ì¡´ì¬**í•œë‹¤.

**ğŸ”¹ Forget Set & Retained Set êµ¬ì„±**
- **Forget Set**ì€ **arXiv, GitHub, Booksì™€ ê°™ì€ ë„ë©”ì¸ì—ì„œ Pre-training Dataë¥¼ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ êµ¬ì„±**í•œë‹¤.
- **Retained Set**ì˜ ì „ì²´ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ë¯€ë¡œ,  
  **Retain ì§‘í•©ì—ì„œ 1K Sequenceë¥¼ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ General Setì„ ìƒì„±**í•œë‹¤.